{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Spark SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of large datasets is being performed at\n",
    "an unprecedented frequency. Several technologies have been\n",
    "developed to do so, offering a variety of solutions and drawbacks\n",
    "related to the processing of different data types and\n",
    "data processing requirements. \n",
    "\n",
    "This notebook implements SparkSQL in order to solve a series of questions by using a data set regarding air polution in the USA.\n",
    "In the report, we compared the performance\n",
    "of five different technologies – MapReduce, Spark RDD,\n",
    "SparkDF, Spark SQL and Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ggpt9\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.1) Which states have more/less monitors? (Rank states!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+\n",
      "|state_code|           StateName|NumberOfMonitors|\n",
      "+----------+--------------------+----------------+\n",
      "|        01|           [Alabama]|              31|\n",
      "|        02|            [Alaska]|              12|\n",
      "|        04|           [Arizona]|              38|\n",
      "|        05|          [Arkansas]|              11|\n",
      "|        06|        [California]|             170|\n",
      "|        08|          [Colorado]|              51|\n",
      "|        09|       [Connecticut]|              15|\n",
      "|        80| [Country Of Mexico]|              18|\n",
      "|        10|          [Delaware]|               6|\n",
      "|        11|[District Of Colu...|               5|\n",
      "|        12|           [Florida]|              55|\n",
      "|        13|           [Georgia]|              35|\n",
      "|        15|            [Hawaii]|               5|\n",
      "|        16|             [Idaho]|              17|\n",
      "|        17|          [Illinois]|              49|\n",
      "|        18|           [Indiana]|              52|\n",
      "|        19|              [Iowa]|              18|\n",
      "|        20|            [Kansas]|              37|\n",
      "|        21|          [Kentucky]|              34|\n",
      "|        22|         [Louisiana]|              41|\n",
      "+----------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    lines = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./epa_hap_daily_summary-small.csv')\n",
    "    lines.createOrReplaceTempView('lines')\n",
    "    t1 = spark.sql(\"SELECT state_code, collect_set(state_name) AS StateName, count(DISTINCT latitude+longitude) AS NumberOfMonitors FROM lines GROUP BY state_code ORDER BY StateName ASC\")\n",
    "    #t1.createGlobalTempView(\"nmps\")\n",
    "    t1.show()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.2) Which counties have the best/worst air quality? (Rank counties considering pollutants’ level!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    county_name|             Value|\n",
      "+---------------+------------------+\n",
      "|         Tipton|            2556.0|\n",
      "|         Nassau|              19.0|\n",
      "|     Columbiana| 7.385690735785953|\n",
      "|CHIHUAHUA STATE|         4.5121875|\n",
      "|       Caldwell| 4.116666666666666|\n",
      "|         Madera|            3.7393|\n",
      "|        Oakland| 2.888877848101266|\n",
      "|          Duval| 2.779460397849462|\n",
      "|         Kearny|2.3753333333333333|\n",
      "|          Bucks|2.3674999999999997|\n",
      "|San Luis Obispo|2.3333333333333335|\n",
      "|      Edgecombe|             2.325|\n",
      "|         Pawnee|2.2941176470588234|\n",
      "|    Westchester|          2.239375|\n",
      "|       Johnston|             2.225|\n",
      "|       Hartford|2.0787055896226416|\n",
      "|      Granville|2.0285714285714285|\n",
      "|         Asotin|             2.025|\n",
      "|         Duplin|2.0000000000000004|\n",
      "|        Boulder|1.9604709012345676|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    lines = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./epa_hap_daily_summary-small.csv')\n",
    "    lines.createOrReplaceTempView('lines')\n",
    "    t1 = spark.sql(\"SELECT county_name, sum(arithmetic_mean)/count(arithmetic_mean) AS Value FROM lines GROUP BY county_name ORDER BY Value DESC\")\n",
    "    t1.show() \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.3) Which states have the best/worst air quality in each year? (Rank states per year\n",
    "considering pollutants’ level!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+----+\n",
      "|state_code|          state_name|             value|year|\n",
      "+----------+--------------------+------------------+----+\n",
      "|        47|         [Tennessee]|170.40093066666668|1990|\n",
      "|        80| [Country Of Mexico]|              8.46|1995|\n",
      "|        26|          [Michigan]| 4.506138716367713|2001|\n",
      "|        25|     [Massachusetts]| 4.305833285714285|1993|\n",
      "|        08|          [Colorado]|4.2250000000000005|2017|\n",
      "|        18|           [Indiana]| 4.098978378378379|1990|\n",
      "|        17|          [Illinois]| 3.911825163398692|1992|\n",
      "|        25|     [Massachusetts]|3.4609906122448986|1994|\n",
      "|        22|         [Louisiana]|3.3643488658536596|1995|\n",
      "|        44|      [Rhode Island]|         3.3635714|1994|\n",
      "|        01|           [Alabama]| 3.226314057971014|1996|\n",
      "|        09|       [Connecticut]|3.0975461538461535|1993|\n",
      "|        25|     [Massachusetts]|3.0246823529411766|1990|\n",
      "|        55|         [Wisconsin]| 2.950483333333333|1994|\n",
      "|        18|           [Indiana]| 2.897225806451613|1993|\n",
      "|        44|      [Rhode Island]|2.7313043478260868|1995|\n",
      "|        10|          [Delaware]|          2.723077|1993|\n",
      "|        18|           [Indiana]| 2.660636363636364|1992|\n",
      "|        42|      [Pennsylvania]| 2.575086206896552|1993|\n",
      "|        11|[District Of Colu...|2.5047463333333333|1995|\n",
      "+----------+--------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    lines = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./epa_hap_daily_summary-small.csv')\n",
    "    lines.createOrReplaceTempView('lines')\n",
    "    t1 = spark.sql(\"SELECT state_code, collect_set(state_name) AS state_name, sum(arithmetic_mean)/count(arithmetic_mean) AS value, LEFT(date_local, 4) AS year FROM lines GROUP BY state_code, year ORDER BY value DESC\")\n",
    "    t1.show()  \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.4) For each state, what is the average distance (in km) of the monitors in that state to\n",
    "the state center? For simplicity, assume that 1 degree of latitude or logitude equals\n",
    "to 111 km. (Monitor dispersion per state!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "| state_name|               hkm|\n",
      "+-----------+------------------+\n",
      "|    Alabama|154.49260106651099|\n",
      "|     Alaska| 578.2547257424047|\n",
      "|    Arizona|164.10851532123158|\n",
      "|   Arkansas|139.88499280806286|\n",
      "| California| 295.9560097738761|\n",
      "|   Colorado|162.79022014593346|\n",
      "|Connecticut|48.010830374129405|\n",
      "|   Delaware|49.430957208141784|\n",
      "|    Florida| 313.9536451547972|\n",
      "|    Georgia|171.86252921573762|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    lines = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./epa_hap_daily_summary-small.csv')\n",
    "    lines.createOrReplaceTempView('lines')\n",
    "    usa = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./usa_states.csv')\n",
    "    usa.createOrReplaceTempView('states')\n",
    "    t2=spark.sql(\"SELECT DISTINCT state_name, address, minlat, latitude, maxlat, minlon, longitude, maxlon, ABS(latitude-(minLat+(maxLat-minLat)/2)) AS dlat, ABS(longitude-(minLon+(maxLon-minLon)/2)) AS dlon FROM lines l, states s WHERE l.state_name=s.name\")\n",
    "    t2.createOrReplaceTempView('t2')\n",
    "    t3=spark.sql(\"SELECT state_name, SQRT(POW(AVG(t.dlat)*111, 2)+POW(AVG(t.dlon)*111, 2)) AS hkm FROM t2 t GROUP BY state_name ORDER BY state_name ASC\")\n",
    "    t3.show(10)   \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "#sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.5) How many sensors there are per quadrant (NW, NE, SE, SW) in each state? To\n",
    "answer this question, you should approximate each state’s area to a rectangle as\n",
    "defined in the file “usa_satates.csv”, and divide that area in 4 quadrants (NW,\n",
    "NE, SE, SW). (Count monitors per sate qudrant!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+---+---+---+-----+\n",
      "|state_code|state_name| NW| NE| SW| SE|total|\n",
      "+----------+----------+---+---+---+---+-----+\n",
      "|        01|   Alabama| 14|  5|  7|  5|   31|\n",
      "|        02|    Alaska|  3|  4|  3|  2|   12|\n",
      "|        04|   Arizona| 10|  2| 10| 16|   38|\n",
      "|        05|  Arkansas|  3|  2|  5|  1|   11|\n",
      "|        06|California| 84|  2| 16| 68|  170|\n",
      "|        08|  Colorado| 17| 25|  5|  4|   51|\n",
      "|        13|   Georgia| 21|  4|  5|  5|   35|\n",
      "|        17|  Illinois|  2| 32| 14|  1|   49|\n",
      "|        18|   Indiana| 18| 18|  8|  8|   52|\n",
      "|        20|    Kansas|  2| 18|  8|  9|   37|\n",
      "+----------+----------+---+---+---+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    lines = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./epa_hap_daily_summary-small.csv')\n",
    "    lines.createOrReplaceTempView('lines')\n",
    "    usa = sqlContext.read.format('com.databricks.spark.csv').options(header=True).load('./usa_states.csv')\n",
    "    usa.createOrReplaceTempView('usa')\n",
    "    states= spark.sql(\"SELECT DISTINCT state_code, state_name, minLat, minLat+ABS((maxLat-minLat)/2) AS centerLat, maxLat, minLon, minLon+((maxLon-minLon)/2) AS centerLon, maxLon, count(DISTINCT latitude+longitude) AS total FROM lines, usa WHERE lines.state_name=usa.name GROUP BY state_code, state_name, minLat, centerLat, maxLat, minLon, CenterLon, maxLon ORDER BY state_code\")\n",
    "    states.createOrReplaceTempView('states')\n",
    "    #states.show(10)\n",
    "    ne= spark.sql(\"SELECT l.state_name, count(DISTINCT latitude+longitude) AS value FROM lines l, states s WHERE l.state_name=s.state_name AND l.latitude>s.centerLat AND l.longitude>s.centerLon GROUP BY l.state_name\")\n",
    "    ne.createOrReplaceTempView('ne')\n",
    "    #ne.show()\n",
    "    nw= spark.sql(\"SELECT l.state_name, count(DISTINCT latitude+longitude) AS value FROM lines l, states s WHERE l.state_name=s.state_name AND l.latitude>s.centerLat AND l.longitude<s.centerLon GROUP BY l.state_name\")\n",
    "    nw.createOrReplaceTempView('nw')\n",
    "    se= spark.sql(\"SELECT l.state_name, count(DISTINCT latitude+longitude) AS value FROM lines l, states s WHERE  l.state_name=s.state_name AND l.latitude<s.centerLat AND l.longitude>s.centerLon GROUP BY l.state_name\")\n",
    "    se.createOrReplaceTempView('se')\n",
    "    sw= spark.sql(\"SELECT l.state_name, count(DISTINCT latitude+longitude) AS value FROM lines l, states s WHERE  l.state_name=s.state_name AND l.latitude<s.centerLat AND l.longitude<s.centerLon GROUP BY l.state_name\")\n",
    "    sw.createOrReplaceTempView('sw')\n",
    "    t2= spark.sql(\"SELECT  l.state_code, l.state_name, nw.value AS NW, ne.value AS NE, sw.value AS SW, se.value AS SE, s.total FROM states s, lines l, usa u, ne, nw, se, sw WHERE l.state_name=ne.state_name AND l.state_name=nw.state_name AND l.state_name=se.state_name AND l.state_name=sw.state_name AND s.state_code=l.state_code GROUP BY l.state_code, l.state_name, nw.value, ne.value, sw.value, se.value, s.total ORDER BY state_name ASC\")\n",
    "    t2.show(10)   \n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
