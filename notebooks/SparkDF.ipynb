{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Project - SparkDF"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Analysis of large datasets is being performed at\n","an unprecedented frequency. Several technologies have been\n","developed to do so, offering a variety of solutions and drawbacks\n","related to the processing of different data types and\n","data processing requirements. \n","\n","This notebook implements SparkDF in order to solve a series of questions by using a data set regarding air polution in the USA.\n","In the report, we compared the performance\n","of five different technologies – MapReduce, Spark RDD,\n","SparkDF, Spark SQL and Hive."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DQKPufOdIOe2"},"source":["## Q.1) Which states have more/less monitors? (Rank states!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHmpz0xqIOe4","outputId":"1f9dcd88-1e18-4b7c-a30e-c87dc1e93941"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+-----+\n","|               state|count|\n","+--------------------+-----+\n","|          California|  170|\n","|               Texas|  133|\n","|           Minnesota|   94|\n","|            Michigan|   92|\n","|                Ohio|   91|\n","|            New York|   67|\n","|      South Carolina|   64|\n","|             Montana|   62|\n","|        Pennsylvania|   61|\n","|             Florida|   55|\n","|             Indiana|   52|\n","|            Colorado|   51|\n","|      North Carolina|   50|\n","|            Illinois|   49|\n","|          Washington|   43|\n","|           Louisiana|   41|\n","|             Arizona|   38|\n","|              Kansas|   37|\n","|             Georgia|   35|\n","|            Kentucky|   34|\n","|              Oregon|   32|\n","|             Alabama|   31|\n","|           Tennessee|   29|\n","|           Wisconsin|   26|\n","|          New Jersey|   24|\n","|             Vermont|   23|\n","|            Oklahoma|   22|\n","|         Mississippi|   21|\n","|               Maine|   21|\n","|       Massachusetts|   19|\n","|            Virginia|   19|\n","|   Country Of Mexico|   18|\n","|          New Mexico|   18|\n","|                Iowa|   18|\n","|            Missouri|   18|\n","|            Maryland|   17|\n","|       New Hampshire|   17|\n","|               Idaho|   17|\n","|         Connecticut|   15|\n","|        Rhode Island|   13|\n","|                Utah|   12|\n","|              Alaska|   12|\n","|            Arkansas|   11|\n","|       West Virginia|   10|\n","|              Nevada|    9|\n","|             Wyoming|    9|\n","|        North Dakota|    7|\n","|        South Dakota|    7|\n","|      Virgin Islands|    6|\n","|            Delaware|    6|\n","|            Nebraska|    6|\n","|         Puerto Rico|    6|\n","|District Of Columbia|    5|\n","|              Hawaii|    5|\n","+--------------------+-----+\n","\n"]}],"source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark.sql import SQLContext\n","\n","spark = SparkSession.builder.master('local[*]').appName('assig').getOrCreate()\n","sc = spark.sparkContext\n","\n","try :\n","    lines = sc.textFile('./epa_hap_daily_summary-small.csv')\n","    l2 = lines.filter( lambda line : len(line) > 0 )\n","    header = l2.first()\n","    txt = l2.filter(lambda line: line != header)\n","    l3 = txt.map( lambda line : line.split(',') )\n","    logRows = l3.map( lambda arr : Row( state = arr[24], coords = arr[5]+arr[6]))\n","\n","    logRowsDF = spark.createDataFrame( logRows )\n","    \n","    stateadd = logRowsDF.groupBy('state').agg(countDistinct('coords').alias('count'))\n","    sasorted= stateadd.sort(col(\"count\").desc())\n","    \n","    sasorted.show(100)\n","except Exception as e:\n","    print(e)\n","sc.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kl2KVIVKISBr"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"r7_fWfx1IOe6"},"source":["## Q.2) Which counties have the best/worst air quality? (Rank counties considering pollutants’ level!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdLWOng0IOe6","outputId":"0d6c6338-de08-461f-eb38-f9050c09d7c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+------------------+\n","|              county|       meanOfmeans|\n","+--------------------+------------------+\n","|              Tipton|            2556.0|\n","|              Nassau|              19.0|\n","|          Columbiana| 7.385690735785953|\n","|     CHIHUAHUA STATE|         4.5121875|\n","|            Caldwell| 4.116666666666667|\n","|              Madera|            3.7393|\n","|             Oakland| 2.888877848101266|\n","|               Duval|2.7794603978494625|\n","|              Kearny|2.3753333333333333|\n","|               Bucks|2.3674999999999997|\n","|     San Luis Obispo|2.3333333333333335|\n","|           Edgecombe|             2.325|\n","|              Pawnee|2.2941176470588234|\n","|         Westchester|          2.239375|\n","|            Johnston|             2.225|\n","|            Hartford|2.0787055896226416|\n","|           Granville|2.0285714285714285|\n","|              Asotin|             2.025|\n","|              Duplin|               2.0|\n","|             Boulder| 1.960470901234568|\n","|          Crittenden|1.9000000000000001|\n","|              Yancey|               1.9|\n","|         Los Angeles|1.8391350859879247|\n","|           Iberville|1.8213132266666665|\n","|             Caswell|           1.80075|\n","|                Pitt|1.7999999999999998|\n","|             Clinton|1.7583018867924527|\n","|               Wayne|1.7137964212983239|\n","|            Imperial|1.6478600515463917|\n","|             Ozaukee|        1.55733332|\n","|          Stillwater|1.5384615384615385|\n","|           Crow Wing| 1.532046511627907|\n","|                Boyd| 1.485702254901961|\n","|          Gloucester|1.4814285714285715|\n","|           Henderson|1.4295509090909093|\n","|            Kennebec|             1.375|\n","|          Stanislaus|1.2781941269841275|\n","|           Muscatine|            1.2375|\n","|          Deer Lodge|1.2135793388429752|\n","|                Mesa|1.2109524924242425|\n","|             Spokane|1.1469733510638298|\n","|    East Baton Rouge|1.1444728456659612|\n","|              Harris|1.1426282723128096|\n","|               Davis|1.1418481240188378|\n","|            Phillips|1.1414093959731544|\n","|             Tuscola|     1.13696140625|\n","|           Nez Perce|1.1226492795389047|\n","|             Bayamon|1.1200363636363637|\n","|           Pipestone|1.0807463414634146|\n","|           Wyandotte|1.0779660104986872|\n","|              Hudson|         1.0754546|\n","|          Williamson|1.0616129032258064|\n","|      St. Louis City|1.0544578440366965|\n","|     Yellow Medicine|1.0511541666666666|\n","|         San Joaquin|1.0142639721936149|\n","|              Miller| 1.011219512195122|\n","|               Ector|1.0014370984405456|\n","|             Sherman|0.9735323943661971|\n","|         St. Charles|0.9709866896551724|\n","|          Otter Tail|0.9641929203539823|\n","|                Kern|0.9552840147731252|\n","|             El Paso|0.9325039253789317|\n","|               Cloud|0.9306044444444446|\n","|          Palm Beach|0.9141428571428573|\n","|              DeKalb|0.9125199496626882|\n","|              Itasca|0.8895128205128204|\n","|                Lake|0.8883477899216881|\n","|            Nicollet| 0.881576923076923|\n","|           Fairfield|0.8765945608108108|\n","|             Hampden|0.8682349224043717|\n","|               Kings|0.8551156355455565|\n","|              Fresno|0.8499170323179771|\n","|           Sherburne|0.8408257575757577|\n","|           Milwaukee|0.8367436675461741|\n","|             Hampton|0.8288424908424908|\n","|           Multnomah|0.8102990317052269|\n","|             Tolland|0.8060031923076922|\n","|                Park|0.8035137527114968|\n","|             Cowlitz| 0.802640243902439|\n","|BAJA CALIFORNIA N...|0.8018910386965376|\n","|               Caddo|             0.798|\n","|             Bristol|0.7900226190476192|\n","|            Rockdale|0.7900198421052631|\n","|            Sedgwick|0.7691381596587447|\n","|             Allegan|0.7629881891891892|\n","|           Merrimack|0.7562957446808511|\n","|              Lehigh|0.7544444444444444|\n","|              Wright|0.7452957746478873|\n","|           Riverside| 0.741725700934578|\n","|            Caroline|0.7399637916666667|\n","|           Kalamazoo|0.7357420124223601|\n","|             Greeley|0.7271684210526316|\n","|              Denver|0.7212535775510206|\n","|              Camden|0.7198732414075286|\n","|                Cook|0.7069328430469428|\n","|           Caledonia|0.7054105882352942|\n","|            Morrison|0.7011904761904763|\n","|              Ottawa|0.6909859050445101|\n","|         Barceloneta| 0.687767955801105|\n","|            Trumbull| 0.660764705882353|\n","+--------------------+------------------+\n","only showing top 100 rows\n","\n"]}],"source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","\n","spark = SparkSession.builder.master('local[*]').appName('assig').getOrCreate()\n","sc = spark.sparkContext\n","\n","try :\n","    lines = sc.textFile('./epa_hap_daily_summary-small.csv')\n","    l2 = lines.filter( lambda line : len(line) > 0 )\n","    header = l2.first()\n","    txt = l2.filter(lambda line: line != header)\n","    l3 = txt.map( lambda line : line.split(',') )\n","    logRows = l3.map( lambda arr : Row( county = arr[25], mean=arr[16]))\n","    logRowsDF = spark.createDataFrame( logRows )\n","    stateadd = logRowsDF.groupBy('county').agg(count('mean').alias('nrecords'), sum('mean').alias('totalmean'))\n","    new_df = stateadd.withColumn(\"meanOfmeans\", (F.col(\"totalmean\") / F.col(\"nrecords\")))\n","    sasorted= new_df.sort(col(\"meanOfmeans\").desc())\n","    final= sasorted.drop('nrecords', 'totalmean')\n","    final.show(100)\n","except Exception as e:\n","    print(e)\n","sc.stop()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NuCmXjHAIOe7"},"source":["## Q.3) Which states have the best/worst air quality in each year? (Rank states per year\n","considering pollutants’ level!)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"de5wix8wIOe7","outputId":"c1d0f1ed-30b8-4f14-f45e-d532ddde02f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+------------------+\n","|               state|year|       meanOfmeans|\n","+--------------------+----+------------------+\n","|           Tennessee|1990|170.40093066666665|\n","|   Country Of Mexico|1995|              8.46|\n","|            Michigan|2001| 4.506138716367713|\n","|       Massachusetts|1993| 4.305833285714285|\n","|            Colorado|2017|4.2250000000000005|\n","|             Indiana|1990| 4.098978378378379|\n","|            Illinois|1992| 3.911825163398692|\n","|       Massachusetts|1994|3.4609906122448977|\n","|           Louisiana|1995| 3.364348865853659|\n","|        Rhode Island|1994|3.3635714000000005|\n","|             Alabama|1996| 3.226314057971015|\n","|         Connecticut|1993|3.0975461538461535|\n","|       Massachusetts|1990|3.0246823529411766|\n","|           Wisconsin|1994|2.9504833333333336|\n","|             Indiana|1993|2.8972258064516128|\n","|        Rhode Island|1995|2.7313043478260868|\n","|            Delaware|1993|          2.723077|\n","|             Indiana|1992|2.6606363636363635|\n","|        Pennsylvania|1993|2.5750862068965517|\n","|District Of Columbia|1995|2.5047463333333333|\n","|           Wisconsin|1995|2.5022444333333334|\n","|           Wisconsin|1998|2.4918918918918918|\n","|         Connecticut|1998|2.3851474516129034|\n","|   Country Of Mexico|1993|              2.38|\n","|         Connecticut|1995|2.2217777749999996|\n","|       Massachusetts|1995|2.2107647272727275|\n","|           Louisiana|1993|2.1176116250000003|\n","|       Massachusetts|1992|2.1174999999999997|\n","|         Connecticut|1997|        2.09315096|\n","|      North Carolina|1995|2.0843603636363635|\n","|            Delaware|1994|          2.082609|\n","|             Montana|1990|2.0686790073529413|\n","|      South Carolina|2016| 2.062485576923077|\n","|        Rhode Island|1997|        2.03347144|\n","|          California|1999|1.9116508577648772|\n","|        Pennsylvania|1996|1.9112836666666666|\n","|           Wisconsin|1999|1.8989440344827586|\n","|                Ohio|2013| 1.858735447154472|\n","|          New Jersey|1997|1.8481895609756098|\n","|         Puerto Rico|2006|1.8463026666666666|\n","|   Country Of Mexico|2004|1.8227272727272732|\n","|       Massachusetts|1998|1.8213961445783133|\n","|   Country Of Mexico|2003|1.8089999999999997|\n","|        Pennsylvania|1994|1.8054421904761901|\n","|             Alabama|1994|1.7862802040816326|\n","|             Georgia|2000|1.7784729333333336|\n","|         Connecticut|2000|1.7584616363636365|\n","|           Louisiana|2007| 1.705220930232558|\n","|        Rhode Island|1996|1.7016831034482756|\n","|   Country Of Mexico|2005|1.6990322580645159|\n","|           Wisconsin|1997| 1.671547595238095|\n","|             Indiana|1991|1.6614237288135594|\n","|       Massachusetts|1996|1.6282544833333332|\n","|          California|1997|1.6016017594670395|\n","|             Montana|1996|1.6004670833333339|\n","|             Georgia|1999|1.5956955925925924|\n","|         Connecticut|1992|            1.5926|\n","|          California|1995| 1.587400337876614|\n","|       Massachusetts|1997| 1.562034396551724|\n","|          California|1998|1.5568169675745778|\n","|              Kansas|1998|1.5540454545454545|\n","|              Oregon|1999|1.5218113372093023|\n","|               Texas|1990|1.4824716546762589|\n","|               Texas|1996| 1.481970256559767|\n","|          California|1996|1.4724994675658305|\n","|        Pennsylvania|1995| 1.466416666666667|\n","|        Pennsylvania|1997|1.4504485112781955|\n","|      North Carolina|1997|1.3985104938271604|\n","|             Alabama|1993|1.3672750000000002|\n","|            New York|1990| 1.362972027972028|\n","|           Louisiana|1996| 1.355698924731183|\n","|      North Carolina|1996|1.3548035955056181|\n","|               Texas|1992|1.3476657142857142|\n","|                Utah|2016|1.3463211180124222|\n","|   Country Of Mexico|1998|        1.34476125|\n","|       Massachusetts|1999|1.3221718219178082|\n","|           Wisconsin|1996| 1.309747846153846|\n","|             Indiana|2008|1.3090953023839393|\n","|               Texas|1997| 1.302157374903773|\n","|            Michigan|2004|1.2914282101359706|\n","|             Florida|1990|1.2765626315789476|\n","|            Delaware|1995|1.2724416666666667|\n","|             Indiana|1995|1.2713646853932583|\n","|   Country Of Mexico|2006|1.2554999999999998|\n","|            Michigan|2010|1.2337054013377924|\n","|            Michigan|2011|1.2318182529118133|\n","|            New York|1998|1.2225324434389142|\n","|            Kentucky|2007|1.2216650922509225|\n","|         Connecticut|1999|1.2134863055555556|\n","|               Texas|1995|1.2043757397820163|\n","|         Connecticut|1996|1.2037766818181816|\n","|          California|2000|1.2019926832034018|\n","|           Louisiana|1991|1.2000000000000002|\n","|      North Carolina|1998|1.1941412244897958|\n","|             Alabama|1995|1.1800432307692308|\n","|        Pennsylvania|1999|1.1628983693693695|\n","|              Kansas|1990|1.1408154574132492|\n","|   Country Of Mexico|2008|1.1300000000000001|\n","|          New Jersey|1999|1.1121644444444443|\n","|   Country Of Mexico|1997|1.0841138461538462|\n","+--------------------+----+------------------+\n","only showing top 100 rows\n","\n"]}],"source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","\n","spark = SparkSession.builder.master('local[*]').appName('assig').getOrCreate()\n","sc = spark.sparkContext\n","\n","try :\n","    lines = sc.textFile('./epa_hap_daily_summary-small.csv')\n","    l2 = lines.filter( lambda line : len(line) > 0 )\n","    header = l2.first()\n","    txt = l2.filter(lambda line: line != header)\n","    l3 = txt.map( lambda line : line.split(',') )\n","    logRows = l3.map( lambda arr : Row( state = arr[24], mean=arr[16], year=arr[11].split('-')[0]))\n","    logRowsDF = spark.createDataFrame( logRows )\n","    stateadd = logRowsDF.select('state', 'year', 'mean').groupBy('state', 'year').agg(count('mean').alias('nrecords'), sum('mean').alias('totalmean'))\n","    new_df = stateadd.withColumn(\"meanOfmeans\", (F.col(\"totalmean\") / F.col(\"nrecords\")))\n","    sasorted= new_df.sort(col(\"meanOfmeans\").desc())\n","    final= sasorted.drop('nrecords', 'totalmean')\n","    final.show(100)\n","except Exception as e:\n","    print(e)\n","sc.stop()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3T7A_fMYIOe7"},"source":["## Q.4) For each state, what is the average distance (in km) of the monitors in that state to\n","the state center? For simplicity, assume that 1 degree of latitude or logitude equals\n","to 111 km. (Monitor dispersion per state!)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8rHpDU-IOe8","outputId":"83412ec5-56b1-45de-cd68-248f8e90055d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------------+\n","|  statename|          avg(hkm)|\n","+-----------+------------------+\n","|    Alabama|167.57779614651386|\n","|     Alaska| 603.6996422410684|\n","|    Arizona|178.87574055612177|\n","|   Arkansas|157.85010472330006|\n","| California| 328.2263813155365|\n","|   Colorado|180.25974203315081|\n","|Connecticut|  49.9897454877856|\n","|   Delaware|  51.5797704802335|\n","|    Florida| 336.5449146570832|\n","|    Georgia|184.34823507347105|\n","|     Hawaii|155.73279515048372|\n","|      Idaho|  289.635073275631|\n","|   Illinois| 224.0676150694105|\n","|    Indiana|177.74195777189746|\n","|       Iowa| 206.5989410368908|\n","|     Kansas| 292.0796841296708|\n","|   Kentucky| 219.9515168075211|\n","|  Louisiana|173.27631230948307|\n","|      Maine| 167.7714354141443|\n","|   Maryland| 89.28759445559284|\n","+-----------+------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import pyspark.sql.functions as F\n","\n","spark = SparkSession.builder.master('local[*]').appName('assig').getOrCreate()\n","sc = spark.sparkContext\n","\n","try :\n","    lines = sc.textFile('./epa_hap_daily_summary-small.csv')\n","    l2 = lines.filter( lambda line : len(line) > 0 )\n","    header = l2.first()\n","    txt = l2.filter(lambda line: line != header)\n","    l3 = txt.map( lambda line : line.split(',') )\n","    \n","    #states info\n","    states= sc.textFile('./usa_states.csv')\n","    statesfil = states.filter( lambda line : len(line) > 0 )\n","    header = statesfil.first()\n","    txt = statesfil.filter(lambda line: line != header)\n","    statesmap = txt.map( lambda line : line.split(',') )\n","    stateRows= statesmap.map( lambda arr : Row( statename = arr[1], minlat=arr[2], maxlat=arr[3], minlon=arr[4], maxlon=arr[5]))\n","    statesDF = spark.createDataFrame( stateRows )\n","    #statesDF.show(10)\n","    #ETL states\n","    new_df2 = statesDF.withColumn(\"meanlat\", F.col(\"minlat\") + abs(F.col(\"maxlat\") - F.col(\"minlat\"))/2)\n","    new_df = new_df2.withColumn(\"meanlon\", F.col(\"minlon\")+((F.col(\"maxlon\") - F.col(\"minlon\"))/2))\n","    #new_df.show(100)\n","    \n","    #evry address info\n","    logRows = l3.map( lambda arr : Row(sn=arr[24], lat=arr[5], lon=arr[6]))\n","    logRowsDF = spark.createDataFrame( logRows )\n","    logd= logRowsDF.distinct()\n","    \n","    #logd.show()\n","\n","    \n","    #join\n","    ndf = logd.join(new_df, [logd.sn==new_df.statename], 'inner')\n","    ndf3= ndf.drop( 'minlat', 'minlon','maxlat', 'maxlon')\n","\n","    #ETL join\n","    ndf1 = ndf3.withColumn(\"dlat\", abs(F.col(\"lat\") - F.col(\"meanlat\")))\n","    ndf2 = ndf1.withColumn(\"dlon\", abs(F.col(\"lon\") - F.col(\"meanlon\")))\n","    hip = ndf2.withColumn(\"h\", sqrt(abs(F.col(\"dlat\"))**2 + abs(F.col(\"dlon\")**2)))\n","    ndf3= hip.drop( 'lat', 'lon','meanlat', 'meanlon')\n","    ndf5= hip.sort(col('h').desc())\n","    ndf6=ndf5.filter(col('lat')!=0)\n","    nhdp = ndf6.withColumn(\"hkm\", F.col('h')*111)\n","    ndf7=nhdp.drop('lat', 'lon', 'sn', 'meanlat', 'meanlon', 'dlon', 'dlat', 'h')\n","    ndf4= ndf7.groupBy( 'statename').agg({'hkm': 'avg'}).sort(col('statename'))\n","    ndf4.show()\n","\n","except Exception as e:\n","    print(e)\n","sc.stop()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GRcsRUKzIOe8"},"source":["## Q.5) How many sensors there are per quadrant (NW, NE, SE, SW) in each state? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eydneOqgIOe8","outputId":"162337e7-c792-4cb1-c053-1df5840cec5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+-------+-------+-------+----------+------------+\n","|countNE|        sn|countNW|countSE|countSW|countState|calcAllcount|\n","+-------+----------+-------+-------+-------+----------+------------+\n","|      5|   Alabama|     14|      5|      7|        31|          31|\n","|      4|    Alaska|      3|      2|      3|        12|          12|\n","|      2|   Arizona|     10|     16|     10|        38|          38|\n","|      2|  Arkansas|      3|      1|      5|        11|          11|\n","|      2|California|     84|     68|     16|       170|         170|\n","|     25|  Colorado|     17|      4|      5|        51|          51|\n","|      4|   Georgia|     21|      5|      5|        35|          35|\n","|     32|  Illinois|      2|      1|     14|        49|          49|\n","|     18|   Indiana|     18|      8|      8|        52|          52|\n","|     18|    Kansas|      2|      9|      8|        37|          37|\n","+-------+----------+-------+-------+-------+----------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import pyspark.sql.functions as F\n","\n","spark = SparkSession.builder.master('local[*]').appName('assig').getOrCreate()\n","sc = spark.sparkContext\n","\n","try :\n","    #states info\n","    states= sc.textFile('./usa_states.csv')\n","    statesfil = states.filter( lambda line : len(line) > 0 )\n","    header = statesfil.first()\n","    txt = statesfil.filter(lambda line: line != header)\n","    statesmap = txt.map( lambda line : line.split(',') )\n","    stateRows= statesmap.map( lambda arr : Row( statename = arr[1], minlat=arr[2], maxlat=arr[3], minlon=arr[4], maxlon=arr[5]))\n","    statesDF = spark.createDataFrame( stateRows )\n","    #ETL states\n","    new_df2 = statesDF.withColumn(\"meanlat\", F.col(\"minlat\") + abs(F.col(\"maxlat\") - F.col(\"minlat\"))/2)\n","    new_df = new_df2.withColumn(\"meanlon\", F.col(\"minlon\")+((F.col(\"maxlon\") - F.col(\"minlon\"))/2))\n","    #new_df.show(100)\n","    \n","    \n","    #evry address info\n","    lines = sc.textFile('./epa_hap_daily_summary-small.csv')\n","    l2 = lines.filter( lambda line : len(line) > 0 )\n","    header = l2.first()\n","    txt = l2.filter(lambda line: line != header)\n","    l3 = txt.map( lambda line : line.split(',') )\n","    logRows = l3.map( lambda arr : Row(sc = arr[0], sn=arr[24], coords = arr[5]+arr[6], lat=arr[5], lon=arr[6]))\n","    logRowsDF = spark.createDataFrame( logRows )\n","    logd= logRowsDF.distinct()\n","    aux4= logd.groupBy('sn').agg(count('coords').alias('countState'))\n","    #logd.show(10)\n","    \n","    \n","    #join\n","    ne = logd.join(new_df, [logd.sn==new_df.statename, logd.lat>=new_df.meanlat, logd.lon>=new_df.meanlon], 'inner')\n","    negb= ne.drop( 'minlat', 'minlon','maxlat', 'maxlon')\n","    #negb.show(10)\n","    nef= negb.groupBy('sn').agg(countDistinct('coords').alias('countNE'))\n","    \n","    nw = logd.join(new_df, [logd.sn==new_df.statename, logd.lat>=new_df.meanlat, logd.lon<=new_df.meanlon], 'inner')\n","    nwgb= nw.drop( 'minlat', 'minlon','maxlat', 'maxlon')\n","    #nwgb.show(10)\n","    nwf= nwgb.groupBy('sn').agg(countDistinct('coords').alias('countNW'))\n","    \n","    se = logd.join(new_df, [logd.sn==new_df.statename, logd.lat<=new_df.meanlat, logd.lon>=new_df.meanlon], 'inner')\n","    segb= se.drop( 'minlat', 'minlon','maxlat', 'maxlon')\n","    #segb.show(10)\n","    sef= segb.groupBy('sn').agg(countDistinct('coords').alias('countSE'))\n","     \n","    sw = logd.join(new_df, [logd.sn==new_df.statename, logd.lat<=new_df.meanlat, logd.lon<=new_df.meanlon], 'inner')\n","    swgb= sw.drop( 'minlat', 'minlon','maxlat', 'maxlon')\n","    #swgb.show(10)\n","    swf= swgb.groupBy('sn').agg(countDistinct('coords').alias('countSW'))\n"," \n","    #join2\n","    aux1 = nef.join(nwf, [nef.sn==nwf.sn], 'inner').drop(nwf.sn)\n","    aux2= aux1.join(sef, [aux1.sn==sef.sn], 'inner').drop(sef.sn)\n","    aux3= aux2.join(swf, [aux2.sn==swf.sn], 'inner').drop(swf.sn)\n","    aux5= aux3.join(aux4, aux4.sn==aux3.sn, 'inner').drop(aux4.sn)\n","    table= aux5.withColumn(\"calcAllcount\", F.col(\"countNE\")+F.col(\"countNW\")+F.col(\"countSE\")+F.col(\"countSW\"))\n","    table_sorted = table.sort(col(\"sn\"))\n","    table_sorted.show(10)\n","    \n","except Exception as e:\n","    print(e)\n","sc.stop()\n"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"97b38eea4165be9112a944b0d5579ed7c1fbf7fb62bf30de8c17a759001bc10e"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
